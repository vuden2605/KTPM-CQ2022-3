# üöÄ Crypto Data Pipeline - News Sentiment + Price Analysis

Pipeline ƒë·ªÉ k·∫øt h·ª£p d·ªØ li·ªáu News Sentiment t·ª´ MongoDB v·ªõi Price Data t·ª´ Binance ƒë·ªÉ training AI model d·ª± ƒëo√°n gi√° cryptocurrency.

---

## ‚ú® C√°c c·∫£i ti·∫øn so v·ªõi version c≈©

### üîß Bugs ƒë√£ fix:
1. ‚úÖ **MongoDB URI** - Th√™m authentication ƒë√∫ng format
2. ‚úÖ **aggregate_news()** - X√≥a duplicate `agg_dict`, fix logic
3. ‚úÖ **Division by zero** - Safe division cho t·∫•t c·∫£ ratios
4. ‚úÖ **Empty DataFrame handling** - X·ª≠ l√Ω tr∆∞·ªùng h·ª£p kh√¥ng c√≥ news
5. ‚úÖ **Data type conversion** - Convert numpy types cho MongoDB
6. ‚úÖ **Error handling** - Try-catch cho t·∫•t c·∫£ critical operations

### üéØ T√≠nh nƒÉng m·ªõi:
1. üîÑ **Retry logic** - Auto retry khi Binance API fail
2. üìä **Data validation** - Check quality tr∆∞·ªõc khi save
3. üìù **Logging system** - Track to√†n b·ªô qu√° tr√¨nh
4. üßπ **Data cleaning** - Remove duplicates, outliers
5. üìà **More features** - RSI, momentum, interaction features
6. üéì **Multi-class target** - Th√™m 4-class classification
7. üîç **Better aggregation** - Sentiment momentum, volume ratios

---

## üìã Requirements

```bash
pip install pandas numpy pymongo requests urllib3
```

**Dependencies:**
- pandas >= 1.3.0
- numpy >= 1.21.0
- pymongo >= 4.0.0
- requests >= 2.26.0

---

## ‚öôÔ∏è Setup

### 1. C·∫•u h√¨nh MongoDB

M·ªü file `crypto_data_pipeline.py` v√† thay ƒë·ªïi:

```python
# Line 31-32
MONGO_USERNAME = "YOUR_USERNAME"  # ‚Üê Thay username th·ª±c t·∫ø
MONGO_PASSWORD = "YOUR_PASSWORD"  # ‚Üê Thay password th·ª±c t·∫ø
```

### 2. Ki·ªÉm tra MongoDB Collections

ƒê·∫£m b·∫£o MongoDB c√≥:
- Database: `cryptonews`
- Collection: `News` (ch·ª©a news data)
- Collection: `AI_Training_Data` (s·∫Ω ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông)

### 3. Schema MongoDB News Collection

```json
{
  "PublishedAt": ISODate("2026-01-22T10:00:00Z"),
  "SentimentScore": 0.75,
  "SentimentLabel": "positive",
  "Title": "Bitcoin reaches new high",
  "Url": "https://...",
  "ExtraJson": {
    "isBreaking": true,
    "breakingScore": 8.5
  }
}
```

---

## üéØ Usage

### Basic Usage

```python
from crypto_data_pipeline import run_pipeline

# Run v·ªõi config m·∫∑c ƒë·ªãnh
df = run_pipeline(
    symbol="BTCUSDT",
    interval="1h",
    start_date="2026-01-01",
    end_date="2026-01-22",
    save_to_mongodb=True,
    save_to_csv=True
)
```

### Advanced Usage

```python
# Multi-timeframe analysis
timeframes = ["1h", "4h", "1d"]

for tf in timeframes:
    df = run_pipeline(
        symbol="BTCUSDT",
        interval=tf,
        start_date="2025-01-01",
        end_date="2026-01-22",
        save_to_mongodb=True,
        save_to_csv=True
    )
    print(f"Completed {tf} timeframe: {len(df)} samples")
```

### Multiple Symbols

```python
symbols = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]

for symbol in symbols:
    df = run_pipeline(
        symbol=symbol,
        interval="1h",
        start_date="2026-01-01",
        end_date="2026-01-22"
    )
```

---

## üìä Output Features

### Price Features (15 features)
- `open`, `high`, `low`, `close`, `volume`
- Returns: `price_return_1h`, `price_return_3h`, `price_return_6h`, `price_return_24h`
- Moving averages: `price_ma_6h`, `price_ma_24h`, `price_ma_168h`
- Volatility: `price_volatility_6h`, `price_volatility_24h`
- Volume ratios: `volume_ratio_6h`, `volume_ratio_24h`
- Indicators: `rsi_14`, `high_low_spread`, `price_position`

### Sentiment Features (20+ features)
- Raw: `sentiment_score_mean`, `sentiment_score_std`, `sentiment_score_min`, `sentiment_score_max`
- Counts: `positive_count`, `negative_count`, `neutral_count`, `sentiment_score_count`
- Ratios: `positive_ratio`, `negative_ratio`, `neutral_ratio`
- Moving averages: `sentiment_ma_3h`, `sentiment_ma_6h`, `sentiment_ma_24h`
- Changes: `sentiment_change_1h`, `sentiment_change_3h`, `sentiment_change_6h`
- Momentum: `sentiment_momentum_3h`
- Lags: `sentiment_lag_1h`, `sentiment_lag_2h`, `sentiment_lag_3h`, `sentiment_lag_6h`
- Breaking news: `is_breaking_sum`, `breaking_score_mean`
- Interactions: `sentiment_x_volume`, `sentiment_x_volatility`

### Target Variables (12 targets)
- **Prices**: `target_price_1h`, `target_price_3h`, `target_price_6h`, `target_price_24h`
- **Returns**: `target_return_1h`, `target_return_3h`, `target_return_6h`, `target_return_24h`
- **Binary**: `target_direction_1h`, `target_direction_3h`, `target_direction_6h`, `target_direction_24h`
  - 0 = DOWN
  - 1 = UP
- **Multi-class**: `target_class_1h`, `target_class_3h`, `target_class_6h`
  - 0 = Strong Down (< -2%)
  - 1 = Down (0% to -2%)
  - 2 = Neutral/Small Up (0% to +2%)
  - 3 = Strong Up (> +2%)

---

## üìÅ Output Files

### CSV File
```
training_data_BTCUSDT_1h_2026-01-01_to_2026-01-22.csv
```

Ch·ª©a to√†n b·ªô features v√† targets, s·∫µn s√†ng cho training.

### MongoDB Collection
```
Database: cryptonews
Collection: AI_Training_Data
```

M·ªói document ch·ª©a:
- T·∫•t c·∫£ features
- Metadata (symbol, interval, dates)
- Timestamp index

---

## üîç Data Quality Checks

Pipeline t·ª± ƒë·ªông validate:
1. ‚úÖ Sufficient data (>100 samples)
2. ‚úÖ Target distribution (kh√¥ng all 1 class)
3. ‚úÖ Missing values (<50% per column)
4. ‚úÖ No extreme outliers in sentiment scores
5. ‚úÖ Price data continuity

---

## üéì Example: Training an AI Model

```python
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Load data
df = pd.read_csv("training_data_BTCUSDT_1h_2026-01-01_to_2026-01-22.csv")

# Select features
feature_cols = [col for col in df.columns if not col.startswith('target_')]
feature_cols = [col for col in feature_cols if col not in ['timestamp', 'symbol', 'interval']]

X = df[feature_cols].fillna(0)
y = df['target_direction_1h']

# Remove rows with missing targets
mask = ~y.isna()
X, y = X[mask], y[mask]

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Evaluate
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Feature importance
importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nTop 10 Most Important Features:")
print(importance.head(10))
```

---

## üêõ Troubleshooting

### Issue 1: MongoDB Connection Failed
```
‚úó MongoDB connection failed: ServerSelectionTimeoutError
```
**Solution:**
- Check username/password
- Verify MongoDB cluster URL
- Check IP whitelist in MongoDB Atlas

### Issue 2: No News Data
```
‚ö† No news found between ... and ...
```
**Solution:**
- Check `PublishedAt` field exists
- Verify date range has data
- Pipeline will continue with price data only

### Issue 3: Binance Rate Limit
```
HTTP Error: 429 Too Many Requests
```
**Solution:**
- ƒê√£ c√≥ retry logic t·ª± ƒë·ªông
- Increase sleep time in line 121
- Use smaller date ranges

### Issue 4: Empty Dataset
```
Insufficient data: only 50 samples
```
**Solution:**
- Increase date range
- Check if Binance has data for that symbol/interval
- Verify interval format: "1h", "4h", "1d" (lowercase)

---

## üìà Performance Tips

### 1. Large Date Ranges
Cho date range > 1 nƒÉm, split th√†nh chunks:
```python
from datetime import datetime, timedelta

start = datetime(2024, 1, 1)
end = datetime(2026, 1, 22)
chunk_size = timedelta(days=90)  # 3 months

current = start
all_dfs = []

while current < end:
    chunk_end = min(current + chunk_size, end)
    
    df = run_pipeline(
        symbol="BTCUSDT",
        interval="1h",
        start_date=current.strftime("%Y-%m-%d"),
        end_date=chunk_end.strftime("%Y-%m-%d"),
        save_to_mongodb=False,
        save_to_csv=False
    )
    
    if df is not None:
        all_dfs.append(df)
    
    current = chunk_end

# Combine all chunks
final_df = pd.concat(all_dfs)
final_df.to_csv("complete_dataset.csv")
```

### 2. Memory Optimization
Cho dataset c·ª±c l·ªõn:
```python
# Use chunks
chunksize = 10000
for chunk in pd.read_csv("large_file.csv", chunksize=chunksize):
    # Process chunk
    pass
```

---

## üìù Changelog

### Version 2.0 (Current)
- ‚úÖ Fixed all critical bugs
- ‚úÖ Added retry logic
- ‚úÖ Improved error handling
- ‚úÖ Added data validation
- ‚úÖ More features (RSI, momentum, etc.)
- ‚úÖ Multi-class targets
- ‚úÖ Better logging
- ‚úÖ MongoDB type conversion

### Version 1.0 (Original)
- Basic pipeline
- Price + sentiment features
- Binary classification only

---

## ü§ù Contributing

N·∫øu b·∫°n t√¨m th·∫•y bugs ho·∫∑c c√≥ suggestions:
1. Test thoroughly
2. Document changes
3. Add error handling
4. Update README

---

## üìû Support

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:
1. Check logs carefully
2. Verify MongoDB connection
3. Test with small date range first
4. Check Binance API status

---

## ‚ö†Ô∏è Important Notes

1. **API Rate Limits**: Binance c√≥ rate limit 1200 requests/minute. Pipeline ƒë√£ optimize.
2. **Data Freshness**: Binance data c√≥ delay ~1 second.
3. **News Coverage**: K·∫øt qu·∫£ ph·ª• thu·ªôc v√†o quality v√† coverage c·ªßa news data.
4. **Backfill**: Binance ch·ªâ c√≥ historical data t·ª´ 2017.
5. **MongoDB Size**: Check disk space khi save large datasets.

---

## üìú License

Free to use. Modify as needed for your projects.

---

## üéØ Next Steps

After generating training data:
1. üßπ **Data Cleaning**: Remove outliers, handle missing values
2. üîß **Feature Engineering**: Create more domain-specific features
3. ü§ñ **Model Selection**: Try different algorithms (XGBoost, LSTM, Transformer)
4. üìä **Backtesting**: Validate on out-of-sample data
5. üöÄ **Deployment**: Real-time prediction pipeline

**Good luck with your AI model! üöÄüìà**
